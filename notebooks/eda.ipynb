{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSF Research Awards Abstracts - Exploratory Data Analisys\n",
    "\n",
    "In this notebook I seek high-level data understanding and define the approach to tackle the clustering task to find specific topics that can group them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "This dataset comprises several paper abstracts, one per file, that were furnished by the NSF (National Science Foundation). A sample abstract is shown at the end.\n",
    "\n",
    "Your task is developing an unsupervised model which classifies abstracts into a topic (discover them!). Indeed, your goal is to group abstracts based on their semantic similarity.\n",
    "\n",
    "You can get a sample of abstracts here. Be creative and state clearly your approach. Although we donâ€™t expect accurate results we want to identify your knowledge over traditional and newest method over NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a virtual env (in root directory) and next install dependencies \n",
    "\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to load .xml files to pandas data frame\n",
    "\n",
    "def load_xml_files_to_dataframe(directory: str) -> pd.DataFrame:\n",
    "    # List to hold DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.xml'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # Load the XML file into a DataFrame and append to the list\n",
    "            df = pd.read_xml(file_path)\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data path using environment variable for security \n",
    "# (create .env file on notebooks folder and load using dotenv and os)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "df = load_xml_files_to_dataframe(os.getenv('DATA_PATH'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We explore basic data values and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's view basic data characteristics\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's confirm the total columns\n",
    "print(\"The dataframe has:\", len(df.columns), \"columns in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look statistics from numerical variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note! The Abstract Narration column has 141 null values. This is important because it is the column of interest for creating the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore our column of interest: Abstract narration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AbstractNarration\"].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AbstractNarration\"].tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that rows 13297 and 13298 have the same information! This column has duplicate values, probably because the same work was awarded multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print one full sample\n",
    "df[\"AbstractNarration\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage unique abstracts\n",
    "print(f'Unique abstracts: {(len(df[\"AbstractNarration\"].unique())/len(df[\"AbstractNarration\"])*100):.2f} %')\n",
    "print('unique abstracts:', len(df[\"AbstractNarration\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's visualize numerical columns trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fuction to plot histogram of award amount\n",
    "def plot_award_amounts(column_name, values):\n",
    "    img = plt.figure(figsize=(8, 8))\n",
    "    plt.hist(values, bins=20, edgecolor='black')\n",
    "    plt.title(f'Histogram of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_award_amounts(\"AwardAmount\", numerical_columns['AwardAmount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns[\"AwardAmount\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export column: Abstract narration as a .csv file to develop cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data directory in root and add this path to .env file\n",
    "# next export data in csv format to this folder\n",
    "df[\"AbstractNarration\"].to_csv(os.getenv(\"RAW_DATA_PATH\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final approach\n",
    "\n",
    "1. Only will be use AbstractNarration column because the task is about semantic similarity and not exist any preview label\n",
    "2. For this task the approach is with K-Means algorithm to define the topics.\n",
    "3. This strategy using embeddings model to create vector space. The model to use is all-MiniLM-L6-v2 for good performance and fast implementation.\n",
    "4. The model is from huggingface Hub\n",
    "5. Create training.py and inference.py files inside pipelines folder for production level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. Validate different vector embedding models\n",
    "2. Use pyspark to improve training performance\n",
    "3. Add other columns and compare metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
