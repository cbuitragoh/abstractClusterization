{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train K-Means model using vector embeddings using all-MiniLM-L6-v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load abstracts\n",
    "abstracts = pd.read_csv(os.getenv(\"FINAL_DATA_PATH\"))['AbstractNarration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = abstracts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings from desired model\n",
    "def generate_embeddings(\n",
    "        data: list[str],\n",
    "        model_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate embeddings from a list of abstracts using the SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        abstracts (list): A list of abstract texts.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: An array of embeddings corresponding to the input abstracts.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(data)\n",
    "\n",
    "    return model, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, embeddings = generate_embeddings(data=data, model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering simple approach defining arbitrary cluster number\n",
    "def create_kmeans_model(\n",
    "        embeddings: np.ndarray,\n",
    "        num_clusters: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a KMeans clustering model with the specified number of clusters\n",
    "    and fits it to the given embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): The embeddings to cluster.\n",
    "        num_clusters (int): The number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        sklearn.cluster.KMeans: The fitted KMeans model.\n",
    "        labels: numpy.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(embeddings)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    return kmeans, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans, labels = create_kmeans_model(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the embeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def save_models(kmeans, model):\n",
    "    joblib.dump(kmeans, f'{os.getenv(\"MODELS_PATH\")}\\kmeans_model.pkl')\n",
    "    joblib.dump(model, f'{os.getenv(\"MODELS_PATH\")}\\sentence_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(kmeans, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the abstracts and their cluster labels to a CSV file using environment variable\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def save_clustered_abstracts(data, labels):\n",
    "    clustered_abstracts = pd.DataFrame(columns=['AbstractNarration', 'label'])\n",
    "    clustered_abstracts['AbstractNarration'] = data\n",
    "    clustered_abstracts['label'] = labels\n",
    "    clustered_abstracts.to_csv(os.getenv(\"CLUSTERED_ABSTRACTS_PATH\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_clustered_abstracts(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_clustered_abstracts(abstracts, labels)\n",
    "print(len(data))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "\n",
    "def evaluate_kmeans_model(\n",
    "        data: pd.DataFrame,\n",
    "        labels: pd.Series,\n",
    "        model: KMeans\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a K-Means model using multiple clustering evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "    - data: pd.DataFrame - The dataset (without labels) used for clustering\n",
    "    - labels: pd.Series - The predicted cluster labels for each data point\n",
    "    - model: KMeans - The fitted KMeans model\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inertia (within-cluster sum of squares)\n",
    "    inertia = model.inertia_\n",
    "    \n",
    "    # Silhouette Score\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    \n",
    "    # Calinski-Harabasz Index\n",
    "    calinski_harabasz = calinski_harabasz_score(data, labels)\n",
    "    \n",
    "    # Davies-Bouldin Index\n",
    "    davies_bouldin = davies_bouldin_score(data, labels)\n",
    "    \n",
    "    # Store all metrics in a dictionary\n",
    "    metrics = {\n",
    "        'inertia': inertia,\n",
    "        'silhouette_score': silhouette_avg,\n",
    "        'calinski_harabasz_score': calinski_harabasz,\n",
    "        'davies_bouldin_score': davies_bouldin\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_kmeans_model(embeddings, labels, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "this result show it not a good clusterization. Let's improve modifying numb_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisDataFrame=pd.DataFrame(columns=[\n",
    "    \"num_clusters\",\n",
    "    \"inertia\",\n",
    "    \"silhouette_score\",\n",
    "    \"calinski_harabasz_score\",\n",
    "    \"davies_bouldin_score\"\n",
    "])\n",
    "for i in list([3,5,10,15,20]):\n",
    "    kmeans, labels = create_kmeans_model(embeddings, num_clusters=i)\n",
    "    results = evaluate_kmeans_model(embeddings, labels, kmeans)\n",
    "    new_row = pd.DataFrame({\n",
    "        \"num_clusters\": i,\n",
    "        \"inertia\": results[\"inertia\"],\n",
    "        \"silhouette_score\": results[\"silhouette_score\"],\n",
    "        \"calinski_harabasz_score\": results[\"calinski_harabasz_score\"],\n",
    "        \"davies_bouldin_score\": results[\"davies_bouldin_score\"]\n",
    "    }, index=[0])\n",
    "    analysisDataFrame = pd.concat([analysisDataFrame, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_kmeans_clusters(data, n_clusters=5, use_pca=False):\n",
    "    \"\"\"\n",
    "    Plots the K-Means clustering results.\n",
    "\n",
    "    Args:\n",
    "    - data: array-like, shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    - n_clusters: int, default=3\n",
    "        The number of clusters for K-Means.\n",
    "    - use_pca: bool, default=False\n",
    "        Whether to apply PCA for dimensionality reduction if data has more than 2 features.\n",
    "\n",
    "    Returns:\n",
    "    - None: Shows the plot of clusters and centroids.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If data has more than 2 dimensions and PCA is requested\n",
    "    if use_pca and data.shape[1] > 2:\n",
    "        print(f\"Data has {data.shape[1]} dimensions. Reducing to 2 dimensions using PCA...\")\n",
    "        pca = PCA(n_components=2)\n",
    "        data_2d = pca.fit_transform(data)\n",
    "    else:\n",
    "        data_2d = data\n",
    "\n",
    "    # Step 2: Fit the K-Means model\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(data_2d)\n",
    "\n",
    "    # Step 3: Get predicted cluster labels\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Step 4: Plot the clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(\n",
    "        x=data_2d[:, 0],\n",
    "        y=data_2d[:, 1],\n",
    "        hue=labels,\n",
    "        palette='viridis',\n",
    "        s=100,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Plot centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "   \n",
    "    plt.scatter(\n",
    "        centroids[:, 0],\n",
    "        centroids[:, 1],\n",
    "        s=300,\n",
    "        c='red',\n",
    "        marker='X',\n",
    "        label='Centroids'\n",
    "    )\n",
    "\n",
    "    # Add plot title and labels\n",
    "    plt.title(f'K-Means Clustering Results (n_clusters={n_clusters})')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_clusters(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
